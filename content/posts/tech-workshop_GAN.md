---
title: "勉強会[GAN ～ガンだったりギャンだったり～]"
date: 2018-11-02T11:21:51+09:00
tags: ["勉強会", "会社紹介", "ニューラルネットワーク", "機械学習", "GAN"]
categories: ["里井 嘉真(yoshisato)"]
---

<br>

皆さんこんにちは。<br>
パーソルプロセス＆テクノロジー株式会社のyoshi-satoです。<br>
今週もテクテクの勉強会の様子をお送りいたします。<br>
よろしくお願いします。<br>
<br>
※技術的なブログについてはQiitaでメンバーが各々に書き進めています。<br>
こちらもぜひご覧ください。<br>
<br>
[Qiita - パーソルプロセス&テクノロジー](https://qiita.com/organizations/persol-pt)<br>
<br>
今週の発表者は、AI分野に詳しいテクテク部の桑田さんです。  
テーマは「GAN ～ガンだったりギャンだったり～」でした。
{{< figure src="/images/tech-workshop/20181102/20181102_174721064.jpg" title="" >}}

<br>

---

## テーマ「GAN ～ガンだったりギャンだったり～」
GAN(Generative Adversarial Network)とは、ディープラーニングの仕組みで画像を生成するモデルです。  
私自身AI系の技術には触れたことがなく、GANも今回の桑田さんの発表で初めて知った概念なので、追いかけるのが大変でした。  
<br>

## pix2pix
はじめにGANを利用したアルゴリズムを紹介して頂きました。  
一つ目は、pix2pixと呼ばれるもので、2つの画像のペア、たとえば白黒画像とカラー画像や、  
線画と写真などを学習させると、自動的に画像の類似点を学習し、白黒画像→カラー画像や線が→写真など  
任意の画像を変換した画像を生成することが出来るというものです。　　

{{< figure src="/images/tech-workshop/20181102/pix2pix.JPG" title="" >}}  

線画を猫のカラー画像に変換するデモページもあるそうですが、  

{{< figure src="/images/tech-workshop/20181102/pix2pix2.JPG" title="" >}}  

使い方を誤ると未来永劫拭い去ることの出来ないトラウマを植えつけられることになります。  
<br>  
## vid2vid
vid2vidは、上のpix2pixの動画版のようなもので、静止画ではなく動画を変換することが出来ます。  
下の例のように、左のカラフルな人形が踊っている動画から、右の女性が踊っている動画に変換することが出来ます。  
影まで付いていてリアルです。しかも動きます。  
夢しかない。  

{{< figure src="/images/tech-workshop/20181102/vid2vid.JPG" title="" >}}  

<br>

## GANって何？
ではGANとはいったいなんでしょう？  
<br>
GANとは、データの分類を行う識別モデルとは異なり、データを生成する生成モデルと呼ばれるものの一種です。  
Generative Adversarial Network（敵対生成ネットワーク）の名のとおり、敵対する2つのモデルを用いて学習を行います。  
generatorと呼ばれるモデルと、discriminatorと呼ばれるモデルの2つに分かれており、generatorが乱数を元に生成した偽物の画像と、本物の画像をdiscriminatorに渡し、discriminatorがその画像が本物か偽物かを判断します。  
これを繰り返して学習を行い、discriminatorが真贋判断できない画像を生成することがゴールとなります。  
学習が成功すれば他の手法よりも鮮明な画像が生成できる一方、2つのモデルのうち片方だけが強くなってしまうなど  
学習が不安定で、上手く学習させるためにはテクニックが必要になるそうです。  
また、学習にコスト・時間がかかるというデメリットもあります。  
NVIDIA Tesla V100を8機搭載しているDGX-1という機器（約1400万円!）の使用が推奨されており、それでも学習に数日かかることもあるそうです。  


{{< figure src="/images/tech-workshop/20181102/whatisgan.JPG" title="" >}}  

<br>

## 「ふと」
GANについての説明をして頂いたので、次にくるのは十中八九「GANやってみた」的なスライドだと会議室の誰もが思ったでしょう(たぶん)
。  

{{< figure src="/images/tech-workshop/20181102/foot.JPG" title="" >}}  

「GANを利用しない場合どういった結果になるのか」  
GANを語った上で、GANを使わないという私のような素人の目には暴挙とも映る検証を行います。  

{{< figure src="/images/tech-workshop/20181102/tameshita.JPG" title="" >}}   

検証内容としては、[GANを使ってアニメ「キルミーベイベー」のキャラクターを生成する記事](https://qiita.com/taku-buntu/items/0093a68bfae0b0ff879d)を、GANを使用しない3つの方法で再現するという内容です。  
<br>

#### 方法① 逆畳み込み
大きなイメージから特徴を抽出しつつ小さなイメージに縮小していく畳み込みと呼ばれるアルゴリズムとは逆に、  
小さなイメージを拡大してから畳み込むという方式です。  

{{< figure src="/images/tech-workshop/20181102/Deconvolution.JPG" title="" >}}   

#### 方法② 単純パーセプトロン
入力に対して重み付けを行い、それらの合計が閾値を超えるか超えないかを判断するアルゴリズムです。  

{{< figure src="/images/tech-workshop/20181102/simplePerceptron.JPG" title="" >}}   

#### 方法③ 多層パーセプトロン
パーセプトロンを連結して層を増やしたネットワークです。

{{< figure src="/images/tech-workshop/20181102/multilayerPerceptron.JPG" title="" >}}  

#### 結果

{{< figure src="/images/tech-workshop/20181102/result.JPG" title="" >}}  

3つの方法のどれも人だと分かる形の画像を生成しています。  
なお「キルミーベイベー」というロゴはデータセットのどの画像にも含まれているのでくっきり出ているのだそうです。  
<br>

## お目当てのGAN
上の検証をGANでも試してみたそうです。  
データセットは同じもので、桑田さん自前でGANを実装し、  
Google ColaboratoryのGPUインスタンスで5000回の学習（12時間）した結果がこちらです。  

{{< figure src="/images/tech-workshop/20181102/resultGan.JPG" title="" >}}  

かなりゲイジュツな感じです。  
GANは学習にコストがかかる上になかなか思い通りにならないと仰っていましたが、  
個人的にはこれはこれで壁紙とかにアリかなと思います。  

<br>
<br>

私自身がAI分野の技術に触れたことがなく、GANという概念についても今回初めて聞いたものなので、  
今回の発表を完全に理解できたわけではありませんが、  
ニューラルネットワークの学習の裏側で何が行われているのか興味が出てきたので、  
時間を見つけて調べてみたいと思います。  
<br>
桑田さん、どうもありがとうございました！  
